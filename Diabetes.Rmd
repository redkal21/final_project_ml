---
title: "Diabetes"
author: "RAFA"
date: "2025-12-04"
output: html_document
---


```{r}
#get data

Diabetes

str(Diabetes)


summary(Diabetes)

library(dplyr)
library(tidyverse)


#we check to see if the distrbution of NAS is even throughout every part of the data
#if there is a large disparity like for examppe is some are missing much more of their proportions than others #then we may have to do some imputation or something bc we may not be able to justify simpling dropping all of our NAS


#this shows whether any column has a disproportionate amount of missing values.
na_dist_cols <- Diabetes %>% 
  summarise(across(everything(), ~mean(is.na(.)) * 100)) %>% 
  pivot_longer(everything(), names_to="variable", values_to="pct_missing") %>%
  arrange(desc(pct_missing))


#we see each variable is missing about 10% of its values


#this shows whether any row has a disproportionate amount of missing values.
Rowcheck<-Diabetes %>%
  mutate(n_missing = rowSums(is.na(.)),
         pct_missing = n_missing / ncol(Diabetes) * 100) %>%
  count(pct_missing) %>%
  arrange(desc(pct_missing)) 

#does this mean we can justify dropping rows missing more than 35% of their data? this is like 190 samples

#maybe then we can populate the other values with the averages of each column




#we need to check if the distribution of missing values is matches the distribution of the data for that column

library(tidyverse)

numeric_vars <- Diabetes %>% select(where(is.numeric)) %>% names()

check_dist_deviation <- function(df, var, all_vars, alpha = 0.05) {
  
  miss_ind <- is.na(df[[var]])
  
  # cannot test if no missing or fully missing
  if (all(!miss_ind) || all(miss_ind)) {
    return(tibble(variable = var, deviates = FALSE, min_p = NA_real_))
  }
  
  other_vars <- setdiff(all_vars, var)
  
  pvals <- map_dbl(other_vars, function(o) {
    x_obs  <- df[[o]][!miss_ind]     # distribution when var IS observed
    x_miss <- df[[o]][miss_ind]      # distribution when var IS missing
    
    # require at least 2 unique points in each distribution
    if (length(x_obs) < 5 || length(x_miss) < 5) return(NA_real_)
    
    ks <- suppressWarnings(ks.test(x_obs, x_miss))
    ks$p.value
  })
  
  min_p <- min(pvals, na.rm = TRUE)
  
  tibble(
    variable = var,
    deviates = min_p < alpha,
    min_p = min_p
  )
}

distribution_results <- map_dfr(
  numeric_vars,
  ~check_dist_deviation(Diabetes, .x, numeric_vars)
)


#we see that the distributoon of NAS matches the distrubution of the data for each column
distribution_results


library(reticulate)

```



```{python}

py_install("pandas", pip=TRUE)

pip install pandas

import pandas as pd

Var = pd.read_csv("/Users/rafa/Desktop/PhD/MAchine Learning/FinalProj/Diabetes.csv")

```




Imputing
Ok so we can impute from mean, median, or mode but which makes sense to use?
Well Are the distributions of all of our variables normally distributed because if they are not normally distributed then imputing could introduce error in parts of the distribution that happen to have higher density of NA’s that will be filled by our imputed values. Regardless, I think if the true values for these imputed points are in truth outliers or near the extremes of an approximately normal distribution, then we lose what could be very important data for modelling or interpretation as we send these observations towards the mean, median, mode etc. But what else can we do?

Perhaps We can impute based on inferred covariance???? Like if we look at all our terms and see that column x covaries lets say about 1:1 with column z, then we should be able to impute based off the distribution of column z values we can project onto the curve of the column x value
Like we predict the missing x value by projecting onto the covariance curve for that row’s corresponding z value


So like if we see that bmi covaries strongly with self reported physical activity score 
Then we use the reported activity of the person with missing values to predict the bmi 
This is probably not very useful as a metric bc bmi is more complicated that this but for imputation i think basing it off covariance curves is a solid bet.

We just need to test for covariance among all the relationships


```{r}
library(tidyverse)

# Select numeric columns only
num_df <- Diabetes %>% select(where(is.numeric))

# Compute correlation matrix
cor_mat <- cor(num_df, use = "pairwise.complete.obs")

# Convert to a long dataframe
cor_df <- as.data.frame(as.table(cor_mat)) %>%
  rename(var1 = Var1, var2 = Var2, correlation = Freq)

# Remove self correlations and duplicates (var1 = var2)
#something doesnt feel right here
cor_df_clean <- cor_df %>%
  filter(var1 != var2) %>%
  group_by(pair = paste(pmin(var1, var2), pmax(var1, var2), sep = "_")) %>%
  slice(1) %>%
  ungroup() %>%
  select(-pair)

```
#we see nothing has strong enough covariances to impute with reasonable justififcation









Here is the real start Im going to begin by doing the training test split at 80:20

but i need to make sure that when we split the data we are selecting random variables proportionally for each bar on the histogram right?
so this is called stratified sampling

```{r}
library(tidyverse)
library(randomForest)  # or ranger if you prefer (faster)
library(caret)

set.seed(123)

train_index <- createDataPartition(Diabetes$Diabetes_binary, p = 0.8, list = FALSE)

train <- Diabetes[train_index, ]
test  <- Diabetes[-train_index, ]

Diabeat<- Diabetes %>% filter(!is.na(Diabetes_binary))
# Drop rows with missing outcomes 

str(Diabeat)

binary_vars <- c(
  "HighBP","HighChol","CholCheck","Smoker","Stroke","HeartDiseaseorAttack",
  "PhysActivity","Fruits","Veggies","HvyAlcoholConsump","AnyHealthcare",
  "NoDocbcCost","DiffWalk","Sex","Diabetes_binary"
)

train <- train %>%
  mutate(across(all_of(binary_vars), ~factor(.x)))

```


making the impute function with abilityto impute differently based on each data type
```{r}
library(randomForest)

impute_with_rf <- function(train_df, var_name) {
  
  # rows with missing and non-missing values
  miss_idx <- which(is.na(train_df[[var_name]]))
  
  if (length(miss_idx) == 0) {
    message("No missing values for ", var_name)
    return(train_df)
  }
  
  obs_idx <- which(!is.na(train_df[[var_name]]))
  
  predictors <- setdiff(names(train_df), var_name)
  form <- as.formula(paste(var_name, "~", paste(predictors, collapse = " + ")))
  
  y <- train_df[[var_name]]
  is_numeric <- is.numeric(y)
  
  message("Imputing ", var_name, " via Random Forest ",
          ifelse(is_numeric, "(regression)", "(classification)"))
  
  rf_fit <- randomForest(
    formula  = form,
    data     = train_df[obs_idx, ],
    ntree    = 200,
    na.action = na.omit
  )
  
  pred <- predict(rf_fit, newdata = train_df[miss_idx, ])
  
  train_df[[var_name]][miss_idx] <- pred
  
  return(train_df)
}






```


```{r}
vars_with_na <- train %>%
  select(-Diabetes_binary) %>%         # don't ever impute outcome
  summarise(across(everything(), ~any(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "var", values_to = "has_na") %>%
  filter(has_na) %>%
  pull(var)

vars_with_na



train_imp <- train

for (v in vars_with_na) {
  train_imp <- impute_with_rf(train_imp, v)
}

write.csv(train_imp, "data/train_imputed.csv", row.names = FALSE)

```

