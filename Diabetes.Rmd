---
title: "Diabetes"
author: "RAFA"
date: "2025-12-04"
output: html_document
---

```{r}

#In this chunk, we load the Diabetes dataset and explore it using str() and summary(). This helps us understand the structure of the data, the types of variables, and get an initial sense of missing values or unusual patterns. We also load dplyr and tidyverse for data manipulation.

Diabetes # Quick look at raw dataset

str(Diabetes) # Shows structure: number of rows, columns, and variable types

summary(Diabetes) # Summary statistics

# Load libraries
library(dplyr)
library(tidyverse)

# we explore the missing values in our dataset.
# We check missingness by column, missingness by row, and whether the missingness
# appears random or follows a pattern. This helps us decide whether imputation is safe.

# ---- Missingness by column ----
# Here we calculate the percentage of missing values for each variable.
# This helps us identify columns with high missingness.

na_dist_cols <- Diabetes %>% 
  summarise(across(everything(), ~mean(is.na(.)) * 100)) %>% 
  pivot_longer(everything(), names_to="variable", values_to="pct_missing") %>%
  arrange(desc(pct_missing))

na_dist_cols   # View missing % for each column


#we see each variable is missing about 10% of its values

# ---- Missingness by row ----
# Check how many values are missing per row.
# This identifies individuals who may be missing too much data.

Rowcheck<-Diabetes %>%
  mutate(n_missing = rowSums(is.na(.)), # number of missing values in row
         pct_missing = n_missing / ncol(Diabetes) * 100) %>% # percent missing in that row
  count(pct_missing) %>%
  arrange(desc(pct_missing)) 

Rowcheck   # View distribution of missingness across rows


# ---- KS Test: Check Whether Missingness is Random ----
# We compare distributions of other numeric variables when a variable is missing
# versus when it is not missing. If distributions are similar, missingness is random,
# which supports safe imputation.

library(tidyverse)

# Select numeric variables to run KS tests on
numeric_vars <- Diabetes %>% select(where(is.numeric)) %>% names()

# Function to test whether missingness affects the distribution of other variables
check_dist_deviation <- function(df, var, all_vars, alpha = 0.05) {
  
  miss_ind <- is.na(df[[var]])   # TRUE when variable is missing
  
  # Skip variable if no missing values or all missing values
  if (all(!miss_ind) || all(miss_ind)) {
    return(tibble(variable = var, deviates = FALSE, min_p = NA_real_))
  }
  
  other_vars <- setdiff(all_vars, var)   # compare var to all other numeric variables
  
  # Run KS test comparing distributions when var IS missing vs. NOT missing
  pvals <- map_dbl(other_vars, function(o) {
    x_obs  <- df[[o]][!miss_ind]     # distribution when var is observed
    x_miss <- df[[o]][miss_ind]      # distribution when var is missing
    
    # Need enough observations to run the KS test
    if (length(x_obs) < 5 || length(x_miss) < 5) return(NA_real_)
    
    ks <- suppressWarnings(ks.test(x_obs, x_miss))   # KS test: compares distributions
    ks$p.value
  })
  
  min_p <- min(pvals, na.rm = TRUE)    # lowest p-value across all comparisons
  
  tibble(
    variable = var,
    deviates = min_p < alpha,          # TRUE if missingness significantly changes distributions
    min_p = min_p
  )
}

# Apply KS test across all numeric variables
distribution_results <- map_dfr(
  numeric_vars,
  ~ check_dist_deviation(Diabetes, .x, numeric_vars)
)

# View whether missingness appears random for each variable
distribution_results

# Interpretation:
# Most variables show no major distribution shift when missing,
# meaning missingness behaves mostly randomly and imputation is safe.

library(reticulate) 


```



Imputing Missing Values
To handle missing values, we first considered simple imputation methods such as mean, median, or mode replacement. However, these approaches can distort the data—especially when variables are not normally distributed or when outliers carry important information. We also considered regression-based imputation, where missing values are predicted using other strongly correlated variables. To evaluate whether this was appropriate, we examined the correlation structure among all numeric predictors. Because we did not observe strong correlations, regression-based imputation was not justified. For this reason, we chose to use a machine-learning-based method (Random Forest imputation), which captures complex, nonlinear relationships between variables and provides a more reliable way to estimate missing values.


```{r}
# In this chunk, we check correlations between numeric variables.
# If we found very strong correlations, regression-based imputation might make sense.
# Since correlations are generally weak, we later decide to use Random Forest–based imputation instead.

library(tidyverse)

# Select numeric columns only
num_df <- Diabetes %>% select(where(is.numeric))

# Compute correlation matrix for numeric variables
cor_mat <- cor(num_df, use = "pairwise.complete.obs")

# Convert correlation matrix to a long dataframe: var1 | var2 | correlation
cor_df <- as.data.frame(as.table(cor_mat)) %>%
  rename(var1 = Var1, var2 = Var2, correlation = Freq)

# Remove self-correlations (var1 == var2) and duplicate pairs (keep only one of each pair)
cor_df_clean <- cor_df %>%
  filter(var1 != var2) %>%
  group_by(pair = paste(pmin(var1, var2), pmax(var1, var2), sep = "_")) %>%
  slice(1) %>%
  ungroup() %>%
  select(-pair)

cor_df_clean   # We can inspect this to see if any pairs have very strong correlations

# From this table, we saw that no variable pairs had strong enough correlation
# to justify simple regression-based imputation. This motivates using ML-based imputation instead.

```


```{r}
# In this chunk, we begin preparing the data for modeling by creating an 80/20
# training–test split. We use stratified sampling to make sure the proportion
# of diabetes vs. non-diabetes cases (the outcome variable) remains balanced
# in both training and test sets. We then convert several binary predictors
# into factor variables, which improves how classification models interpret them.

library(tidyverse)
library(randomForest)  # later used for Random Forest imputation
library(caret)         # provides createDataPartition() for stratified splitting

set.seed(123)          # ensures that the train–test split is reproducible

# ---- Stratified Train–Test Split ----
# createDataPartition() splits the data while keeping similar proportions
# of the outcome (Diabetes_binary) in both the training and test sets.
train_index <- createDataPartition(Diabetes$Diabetes_binary, p = 0.8, list = FALSE)

# Create training (80%) and test (20%) subsets
train <- Diabetes[train_index, ]
test  <- Diabetes[-train_index, ]

# Inspect rows that have non-missing outcome values
# (Note: This object is not used in the split above but helps us explore missing outcomes.)
Diabeat <- Diabetes %>% filter(!is.na(Diabetes_binary))

str(Diabeat)  # Quick check of structure after filtering out missing outcomes


# ---- Convert Binary Predictors to Factors ----
# Many variables in this dataset are coded as 0/1 but conceptually represent categories.
# Converting them to factors improves how models treat these variables.
binary_vars <- c(
  "HighBP","HighChol","CholCheck","Smoker","Stroke","HeartDiseaseorAttack",
  "PhysActivity","Fruits","Veggies","HvyAlcoholConsump","AnyHealthcare",
  "NoDocbcCost","DiffWalk","Sex","Diabetes_binary"
)

train <- train %>%
  mutate(across(all_of(binary_vars), ~ factor(.x)))

# After this chunk, we have:
# - a stratified train/test split
# - binary predictors correctly converted to factors (in the training set)
# These steps prepare the data for imputation and later machine-learning models.

```




```{r}
# In this chunk, we define a helper function that performs Random Forest–based imputation
# for a single variable. For any variable with missing values, this function:
# 1) Uses all other variables as predictors,
# 2) Trains a Random Forest model on the rows where that variable is observed,
# 3) Predicts and fills in the missing values.

library(randomForest)

impute_with_rf <- function(train_df, var_name) {
  
  # Find the row indices where this variable is missing
  miss_idx <- which(is.na(train_df[[var_name]]))
  
  # If there are no missing values for this variable, just return the data unchanged
  if (length(miss_idx) == 0) {
    message("No missing values for ", var_name)
    return(train_df)
  }
  
  # Find the row indices where this variable is observed (not missing)
  obs_idx <- which(!is.na(train_df[[var_name]]))
  
  # Use all other variables in the dataset as predictors
  predictors <- setdiff(names(train_df), var_name)
  form <- as.formula(paste(var_name, "~", paste(predictors, collapse = " + ")))
  
  # Check whether the target variable is numeric or not (just for labeling the message)
  y <- train_df[[var_name]]
  is_numeric <- is.numeric(y)
  
  message("Imputing ", var_name, " via Random Forest ",
          ifelse(is_numeric, "(regression)", "(classification)"))
  
  # Fit a Random Forest model using only the rows where this variable is not missing
  rf_fit <- randomForest(
    formula   = form,
    data      = train_df[obs_idx, ],
    ntree     = 200,
    na.action = na.omit      # ignore rows that have NAs in the predictor variables
  )
  
  # Predict the missing values using the trained Random Forest model
  pred <- predict(rf_fit, newdata = train_df[miss_idx, ])
  
  # Fill in the missing entries of this variable with the predicted values
  train_df[[var_name]][miss_idx] <- pred
  
  # Return the updated data frame with imputed values for this variable
  return(train_df)
}

```




```{r}
# In this chunk, we:
# 1) Identify which predictor variables in the training set still have missing values,
# 2) Apply our Random Forest imputation function to each of those variables,

vars_with_na <- train %>%
  select(-Diabetes_binary) %>%         # Exclude the outcome: we never impute Diabetes_binary
  summarise(across(everything(), ~ any(is.na(.)))) %>%  # Check which columns have at least one NA
  pivot_longer(everything(), names_to = "var", values_to = "has_na") %>%  # Turn into long format
  filter(has_na) %>%                  
  pull(var)                            

vars_with_na   # List of variables that will be imputed


# Create a copy of the training data that we will modify with imputed values
train_imp <- train

# Loop over each variable with missing values and impute it using our Random Forest function
for (v in vars_with_na) {
  train_imp <- impute_with_rf(train_imp, v)
}

# Save the imputed training set to a CSV file so we can use it later for modeling
write.csv(train_imp, "data/train_imputed.csv", row.names = FALSE)

```



## GAM Modeling
```{r}
library(mgcv)

# Load the imputed training data
train_imp <- read.csv("train_imputed.csv")

# Make sure outcome is a factor
train_imp$Diabetes_binary <- as.factor(train_imp$Diabetes_binary)

# Quick check
summary(train_imp$Diabetes_binary)

# Check columns
names(train_imp)

```
```{r}
# First simple GAM model:

gam_simple <- gam(
  Diabetes_binary ~ s(Age) + s(BMI),
  data   = train_imp,
  family = binomial
)

# Look at the model summary
summary(gam_simple)

```

```{r}
# Plot the smooth effects of Age and BMI
plot(gam_simple, shade = TRUE, seWithMean = TRUE)
```


```{r}
# Turn these into factors (they're categorical scales, not true continuous)
train_imp$GenHlth   <- as.factor(train_imp$GenHlth)
train_imp$Education <- as.factor(train_imp$Education)
train_imp$Income    <- as.factor(train_imp$Income)

# Also make sure the other categorical ones are factors
cat_vars <- c(
  "HighBP","HighChol","CholCheck","Smoker","Stroke",
  "HeartDiseaseorAttack","PhysActivity","Fruits","Veggies",
  "HvyAlcoholConsump","AnyHealthcare","NoDocbcCost",
  "DiffWalk","Sex"
)

train_imp[cat_vars] <- lapply(train_imp[cat_vars], as.factor)

str(train_imp$GenHlth)
str(train_imp$Education)
str(train_imp$Income)

```



```{r}
# Full GAM: 
# smooth for truly continuous variables,
# factors for the rest.

gam_full <- gam(
  Diabetes_binary ~ 
    s(Age) + 
    s(BMI) + 
    s(MentHlth) + 
    s(PhysHlth) +
    GenHlth + Education + Income +   # now as factors
    HighBP + HighChol + CholCheck + Smoker + Stroke +
    HeartDiseaseorAttack + PhysActivity + Fruits + Veggies +
    HvyAlcoholConsump + AnyHealthcare + NoDocbcCost +
    DiffWalk + Sex,
  data   = train_imp,
  family = binomial,
  method = "REML"
)

summary(gam_full)
```



```{r}
## Step 3 – Plot smooth terms again
plot(gam_full, pages = 1, shade = TRUE, seWithMean = TRUE)
```

